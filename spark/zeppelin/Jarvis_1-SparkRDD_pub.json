{"paragraphs":[{"text":"%md\n### How to interact with Spark\n\nTo start a Spark job (either single JVM or distributed mode), we can simply execute `bin/spark-shell` cmd which will launch JVM which has the Spark Job running. The entry point of the Spark Job (JVM) is called SparkSession which allows user/application interact with the Spark Job. For instance, you can create RDDs through SparkSession from a exiting Scala collection (e.g. Array, List, Set) or from a data source (e.g. read text from HDFS to RDD)\n\n> Compare SparkContext and Spark Sessionhttps://databricks.com/blog/2016/08/15/how-to-use-sparksession-in-apache-spark-2-0.html https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.sql.SparkSession https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.SparkContext\n\nAlternatively, we can use Zeppelin to create a Spark Job and Zeppline will automatically create a SparkSession (`spark`) and a SparkContext (`sc`) for you.\n","user":"anonymous","dateUpdated":"2020-03-17T15:39:30+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>How to interact with Spark</h3>\n<p>To start a Spark job (either single JVM or distributed mode), we can simply execute <code>bin/spark-shell</code> cmd which will launch JVM which has the Spark Job running. The entry point of the Spark Job (JVM) is called SparkSession which allows user/application interact with the Spark Job. For instance, you can create RDDs through SparkSession from a exiting Scala collection (e.g. Array, List, Set) or from a data source (e.g. read text from HDFS to RDD)</p>\n<blockquote>\n  <p>Compare SparkContext and Spark <a href=\"Sessionhttps://databricks.com/blog/2016/08/15/how-to-use-sparksession-in-apache-spark-2-0.html\">Sessionhttps://databricks.com/blog/2016/08/15/how-to-use-sparksession-in-apache-spark-2-0.html</a> <a href=\"https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.sql.SparkSession\">https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.sql.SparkSession</a> <a href=\"https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.SparkContext\">https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.SparkContext</a></p>\n</blockquote>\n<p>Alternatively, we can use Zeppelin to create a Spark Job and Zeppline will automatically create a SparkSession (<code>spark</code>) and a SparkContext (<code>sc</code>) for you.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1584459570136_-1393892327","id":"20190921-014743_1530188134","dateCreated":"2020-03-17T15:39:30+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:352"},{"text":"%spark\n\n//Spark session and sparkContext are loaded automatically\nprintln(spark.version.to)\nprintln(spark)\n\n//The following two lines point to the same SparkContext@2a695829 where @2a695829 is the memory address\nprintln(spark.sparkContext)\nprintln(sc)\n","user":"anonymous","dateUpdated":"2020-04-06T19:53:21+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"2.3.4\norg.apache.spark.sql.SparkSession@74aec622\norg.apache.spark.SparkContext@1cceeb19\norg.apache.spark.SparkContext@1cceeb19\n"}]},"apps":[],"jobName":"paragraph_1584459570148_1404192845","id":"20190921-013657_404311467","dateCreated":"2020-03-17T15:39:30+0000","dateStarted":"2020-04-06T19:53:21+0000","dateFinished":"2020-04-06T19:53:22+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:353"},{"text":"%md","user":"anonymous","dateUpdated":"2020-03-17T15:39:30+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1584459570148_-244334104","id":"20190922-220218_788870347","dateCreated":"2020-03-17T15:39:30+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:354"},{"text":"%md\n### Creating RDDs from Scala collections\n\nWe can use `sc.parallelize` method to create RDDs from Scala collections\n(Note: `parallelize` is not available in the `SparkSession`. However, you can use `SparkSession.sparkContext.parallelize` instead)\n\nhttps://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.SparkContext","user":"anonymous","dateUpdated":"2020-03-17T15:39:30+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Creating RDDs from Scala collections</h3>\n<p>We can use <code>sc.parallelize</code> method to create RDDs from Scala collections<br/>(Note: <code>parallelize</code> is not available in the <code>SparkSession</code>. However, you can use <code>SparkSession.sparkContext.parallelize</code> instead)</p>\n<p><a href=\"https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.SparkContext\">https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.SparkContext</a></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1584459570149_689598543","id":"20190921-022812_325072599","dateCreated":"2020-03-17T15:39:30+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:355"},{"text":"%spark\n//Create RDDs from Scala collections\nval lsRdd = sc.parallelize(List(1,2,3,4,5))\n\n//number of items in lsRdd\nval count = lsRdd.count\n\n//first element in lsRdd\nval firstE = lsRdd.first\n\n//Number of partitions\nval partitionsNum = lsRdd.partitions.length\n\n//Manipulating lsRDD\nval dupRdd = lsRdd.flatMap(i => List.fill(i)(i))\nval dupArray = dupRdd.collect\nval evens = dupRdd.filter(_%2 == 0).collect","user":"anonymous","dateUpdated":"2020-04-06T19:53:29+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"lsRdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[17] at parallelize at <console>:27\ncount: Long = 5\nfirstE: Int = 1\npartitionsNum: Int = 2\ndupRdd: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[18] at flatMap at <console>:29\ndupArray: Array[Int] = Array(1, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 5)\nevens: Array[Int] = Array(2, 2, 4, 4, 4, 4)\n"}]},"apps":[],"jobName":"paragraph_1584459570150_1732966507","id":"20190921-020350_225494359","dateCreated":"2020-03-17T15:39:30+0000","dateStarted":"2020-04-06T19:53:29+0000","dateFinished":"2020-04-06T19:53:33+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:356"},{"text":"%md","user":"anonymous","dateUpdated":"2020-03-17T15:39:30+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1584459570152_1611336683","id":"20190922-220230_613999600","dateCreated":"2020-03-17T15:39:30+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:357"},{"text":"%md\n### Creating RDDs from Data source\n\n- `ssh` to dataproc master node\n- Download `online-retail-dataset.txt` dataset [link](https://github.com/databricks/Spark-The-Definitive-Guide/blob/master/data/retail-data/all/online-retail-dataset.csv)\n- Upload `online-retail-dataset.txt` to a HDFS location (e.g. hdfs dfs -put ...)\n- Inspect the dataset using spark RDD (see below Spark code)\n- Discuss why there are some lines that have more than 8 columns (hint: csv format)\n- Discuss some possible solutions ","user":"anonymous","dateUpdated":"2020-03-17T15:39:30+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Creating RDDs from Data source</h3>\n<ul>\n  <li><code>ssh</code> to dataproc master node</li>\n  <li>Download <code>online-retail-dataset.txt</code> dataset <a href=\"https://github.com/databricks/Spark-The-Definitive-Guide/blob/master/data/retail-data/all/online-retail-dataset.csv\">link</a></li>\n  <li>Upload <code>online-retail-dataset.txt</code> to a HDFS location (e.g. hdfs dfs -put &hellip;)</li>\n  <li>Inspect the dataset using spark RDD (see below Spark code)</li>\n  <li>Discuss why there are some lines that have more than 8 columns (hint: csv format)</li>\n  <li>Discuss some possible solutions</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1584459570157_-1832412535","id":"20190920-182511_1653833929","dateCreated":"2020-03-17T15:39:30+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:358"},{"text":"%spark\nval retailRDD = sc.textFile(\"hdfs:///user/manpreetk0294/datasets/online_retail/online-retail-dataset.csv\")\n\n//count number of elements in the RDD\nretailRDD.count\n\n//understand what does each element look like in RDD\nval firstE = retailRDD.first()\n\n//find out does withReplacement mean from the scal doc https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.RDD\nretailRDD.takeSample(false, 10, 1).foreach(println)","user":"anonymous","dateUpdated":"2020-04-06T19:53:36+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true,"editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"retailRDD: org.apache.spark.rdd.RDD[String] = hdfs:///user/manpreetk0294/datasets/online_retail/online-retail-dataset.csv MapPartitionsRDD[21] at textFile at <console>:25\nres30: Long = 541910\nfirstE: String = InvoiceNo,StockCode,Description,Quantity,InvoiceDate,UnitPrice,CustomerID,Country\n552290,21430,SET/3 RED GINGHAM ROSE STORAGE BOX,1,5/8/2011 13:32,3.75,16007,United Kingdom\n578349,22636,CHILDS BREAKFAST SET CIRCUS PARADE,2,11/24/2011 9:50,8.5,14539,United Kingdom\n537666,84917,WHITE HAND TOWEL WITH BUTTERFLY,1,12/7/2010 18:36,4.21,,United Kingdom\n547021,20749,ASSORTED COLOUR MINI CASES,2,3/18/2011 15:43,7.95,13046,United Kingdom\n553718,35809A,ENAMEL PINK TEA CONTAINER,1,5/18/2011 16:14,2.46,,United Kingdom\n557466,21242,RED RETROSPOT PLATE ,8,6/20/2011 13:08,1.69,13815,Germany\n567160,21218,RED SPOTTY BISCUIT TIN,1,9/18/2011 10:35,3.75,14562,United Kingdom\n548975,17003,BROCADE RING PURSE ,108,4/5/2011 11:47,0.29,17596,United Kingdom\n559338,85086A,CANDY SPOT HEART DECORATION,1,7/7/2011 16:30,0.83,,United Kingdom\n546769,22499,WOODEN UNION JACK BUNTING,3,3/16/2011 14:57,5.95,17504,United Kingdom\n"}]},"apps":[],"jobName":"paragraph_1584459570158_-1732019309","id":"20190920-182724_1961848616","dateCreated":"2020-03-17T15:39:30+0000","dateStarted":"2020-04-06T19:53:36+0000","dateFinished":"2020-04-06T19:53:41+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:359"},{"text":"%md\n","user":"anonymous","dateUpdated":"2020-03-17T15:39:30+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1584459570159_-337683084","id":"20190922-220256_1973670371","dateCreated":"2020-03-17T15:39:30+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:360"},{"text":"%md\n### CSV format issue\n- There are some line that have more than 8 columns. Reason being, there some fields that contain comma.SO, spard tends to split the columns when it encounters and comma and resulting in generating extra columns\n- **Problem Solution**: It can be solved by using `OpenCSV SerDe` or we can simpe remove commas between double quotes and then remove all double quotes","user":"anonymous","dateUpdated":"2020-03-19T19:01:45+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>CSV format issue</h3>\n<ul>\n  <li>There are some line that have more than 8 columns. Reason being, there some fields that contain comma.SO, spard tends to split the columns when it encounters and comma and resulting in generating extra columns</li>\n  <li><strong>Problem Solution</strong>: It can be solved by using <code>OpenCSV SerDe</code> or we can simpe remove commas between double quotes and then remove all double quotes</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1584459570159_1130470619","id":"20190921-023538_989684097","dateCreated":"2020-03-17T15:39:30+0000","dateStarted":"2020-03-19T19:01:45+0000","dateFinished":"2020-03-19T19:01:47+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:361"},{"text":"%spark\n//Check CSV format\nval splitRdd = retailRDD.map(s => s.split(\",\"))\n\n//Some lines have more than 8 column which indicates a format issue\nval samples = splitRdd.map(arr => arr.length).takeSample(false,15, 11)\n\n//find out how many lines have more than 8 cols\nval lenArrRdd = splitRdd.map(arr => (arr.length, arr))\nlenArrRdd.filter(_._1 != 8).take(3).foreach({case(count, cols) => println(count + \":\" + cols.mkString(\"||\"))})\n","user":"anonymous","dateUpdated":"2020-04-06T19:53:44+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"splitRdd: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[23] at map at <console>:29\nsamples: Array[Int] = Array(8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8)\nlenArrRdd: org.apache.spark.rdd.RDD[(Int, Array[String])] = MapPartitionsRDD[26] at map at <console>:31\n9:536381||82567||\"AIRLINE LOUNGE||METAL SIGN\"||2||12/1/2010 9:41||2.1||15311||United Kingdom\n9:536394||21506||\"FANCY FONT BIRTHDAY CARD|| \"||24||12/1/2010 10:39||0.42||13408||United Kingdom\n9:536520||22760||\"TRAY|| BREAKFAST IN BED\"||1||12/1/2010 12:43||12.75||14729||United Kingdom\n"}]},"apps":[],"jobName":"paragraph_1584459570160_-967428905","id":"20190921-023311_1509488233","dateCreated":"2020-03-17T15:39:30+0000","dateStarted":"2020-04-06T19:53:45+0000","dateFinished":"2020-04-06T19:53:47+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:362"},{"text":"%md\n### Pre-process dataset\n\nWe need to deal with fields that contain commas. e.g. `123,\"second, field\",\"third field\"`. We have seen this csv format issue in Hive, and we solved it using `OpenCSV` SerDe. In this practice, we will simply remove commas between double quotes and then remove all double quotes.\n\n- Removing commas in `Description` field (e.g. \"Apple, Inc\" => \"Apple Inc\")<br>`awk -F'\"' -v OFS='' '{ for (i=2; i<=NF; i+=2) gsub(\",\", \"\", $i) } 1' online-retail-dataset.txt`\n- Remove all double double quotes<br>`sed 's/\"//g' online-retail-dataset.txt`\n- output file: `online-retail-dataset_clean.txt`\n\n### Move file to HDFS\nMove `online-retail-dataset_clean.txt` to HDFS\n\n### Spark RDD Cache\n- https://spark.apache.org/docs/2.3.3/rdd-programming-guide.html#rdd-persistence\n- https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.RDD@cache():RDD.this.type","user":"anonymous","dateUpdated":"2020-03-19T19:40:15+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Pre-process dataset</h3>\n<p>We need to deal with fields that contain commas. e.g. <code>123,&quot;second, field&quot;,&quot;third field&quot;</code>. We have seen this csv format issue in Hive, and we solved it using <code>OpenCSV</code> SerDe. In this practice, we will simply remove commas between double quotes and then remove all double quotes.</p>\n<ul>\n  <li>Removing commas in <code>Description</code> field (e.g. &ldquo;Apple, Inc&rdquo; =&gt; &ldquo;Apple Inc&rdquo;)<br><code>awk -F&#39;&quot;&#39; -v OFS=&#39;&#39; &#39;{ for (i=2; i&lt;=NF; i+=2) gsub(&quot;,&quot;, &quot;&quot;, $i) } 1&#39; online-retail-dataset.txt</code></li>\n  <li>Remove all double double quotes<br><code>sed &#39;s/&quot;//g&#39; online-retail-dataset.txt</code></li>\n  <li>output file: <code>online-retail-dataset_clean.txt</code></li>\n</ul>\n<h3>Move file to HDFS</h3>\n<p>Move <code>online-retail-dataset_clean.txt</code> to HDFS</p>\n<h3>Spark RDD Cache</h3>\n<ul>\n  <li><a href=\"https://spark.apache.org/docs/2.3.3/rdd-programming-guide.html#rdd-persistence\">https://spark.apache.org/docs/2.3.3/rdd-programming-guide.html#rdd-persistence</a></li>\n  <li><a href=\"https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.RDD@cache():RDD.this.type\">https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.RDD@cache():RDD.this.type</a></li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1584459570160_856950704","id":"20190519-113048_765206384","dateCreated":"2020-03-17T15:39:30+0000","dateStarted":"2020-03-19T19:40:15+0000","dateFinished":"2020-03-19T19:40:15+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:363"},{"text":"%sh\nhdfs dfs -cat /user/manpreetk0294/datasets/online_retail/online-retail-dataset.csv | awk -F'\"' -v OFS='' '{ for (i=2; i<=NF; i+=2) gsub(\",\", \"\", $i) } 1' | sed 's/\"//g' > online-retail-dataset_clean.csv","user":"anonymous","dateUpdated":"2020-04-06T19:53:53+0000","config":{"colWidth":4,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/sh","editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1584644821424_1414232644","id":"20200319-190701_1639537617","dateCreated":"2020-03-19T19:07:01+0000","dateStarted":"2020-04-06T19:53:53+0000","dateFinished":"2020-04-06T19:54:00+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:364"},{"text":"%sh\n hdfs dfs -put online-retail-dataset_clean.csv /user/manpreetk0294/datasets/online_retail/\n##hdfs dfs -rm /user/manpreetk0294/datasets/online_retail/online-retail-dataset_clean.csv","user":"anonymous","dateUpdated":"2020-04-06T19:54:12+0000","config":{"colWidth":8,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"put: `/user/manpreetk0294/datasets/online_retail/online-retail-dataset_clean.csv': File exists\n"},{"type":"TEXT","data":"ExitValue: 1"}]},"apps":[],"jobName":"paragraph_1584646842477_-767106465","id":"20200319-194042_808857944","dateCreated":"2020-03-19T19:40:42+0000","dateStarted":"2020-04-06T19:54:12+0000","dateFinished":"2020-04-06T19:54:15+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:365"},{"text":"%sh\nhdfs dfs -ls /user/manpreetk0294/datasets/online_retail/","user":"anonymous","dateUpdated":"2020-04-06T19:54:26+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Found 3 items\n-rw-r--r--   2 manpreetk0294 hadoop     102598 2020-03-22 06:52 /user/manpreetk0294/datasets/online_retail/customers.txt\n-rw-r--r--   2 manpreetk0294 hadoop   45038760 2020-03-18 21:59 /user/manpreetk0294/datasets/online_retail/online-retail-dataset.csv\n-rw-r--r--   2 zeppelin      hadoop   45016675 2020-03-21 18:27 /user/manpreetk0294/datasets/online_retail/online-retail-dataset_clean.csv\n"}]},"apps":[],"jobName":"paragraph_1584646987970_-1671885498","id":"20200319-194307_1559456562","dateCreated":"2020-03-19T19:43:07+0000","dateStarted":"2020-04-06T19:54:26+0000","dateFinished":"2020-04-06T19:54:28+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:366"},{"text":"//Load csv file\n//Lazy evaluation\n//val datasetDir = \"/home/centos/dev/jrvs/bootcamp/hadoop/datasets\"\nval filePath = \"hdfs:///user/manpreetk0294/datasets/online_retail/online-retail-dataset_clean.csv\"\nval retailRDD = sc.textFile(filePath)\n\n//RDD action triggers evaluation (in this case count is an action)\nval count = retailRDD.count\n\n//tip: Use tab key to auto-complete\nval sample3 = retailRDD.takeSample(false, 3, 22)\n\n//Make sure every row has exactly 0 columns\nval longRow = retailRDD.filter(row => row.split(\",\").length != 8 ).count\n\n//Cache RDD since it will be accessed frequently\nretailRDD.cache","user":"anonymous","dateUpdated":"2020-04-06T19:54:33+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"filePath: String = hdfs:///user/manpreetk0294/datasets/online_retail/online-retail-dataset_clean.csv\nretailRDD: org.apache.spark.rdd.RDD[String] = hdfs:///user/manpreetk0294/datasets/online_retail/online-retail-dataset_clean.csv MapPartitionsRDD[29] at textFile at <console>:27\ncount: Long = 541910\nsample3: Array[String] = Array(569457,22329,ROUND CONTAINER SET OF 5 RETROSPOT,1,10/4/2011 11:29,1.65,14606,United Kingdom, 571265,22530,MAGIC DRAWING SLATE DOLLY GIRL ,2,10/16/2011 11:31,0.42,16674,United Kingdom, 563893,90064B,BLACK VINTAGE  CRYSTAL EARRINGS,1,8/19/2011 17:10,3.75,16330,United Kingdom)\nlongRow: Long = 0\nres47: retailRDD.type = hdfs:///user/manpreetk0294/datasets/online_retail/online-retail-dataset_clean.csv MapPartitionsRDD[29] at textFile at <console>:27\n"}]},"apps":[],"jobName":"paragraph_1584459570161_-1958430359","id":"20190519-105016_1691323616","dateCreated":"2020-03-17T15:39:30+0000","dateStarted":"2020-04-06T19:54:33+0000","dateFinished":"2020-04-06T19:54:36+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:367"},{"text":"//Making some utilities and make your life easier :)\n//val printRddNSamples = (rdd: org.apache.spark.rdd.RDD[_], n: Int) => rdd.takeSample(false, n, 22).foreach(println)\n//val printRdd3Samples = (rdd: org.apache.spark.rdd.RDD[_]) => printRddNSamples(rdd, 3)\n//val printRddTopN = (rdd: org.apache.spark.rdd.RDD[_], n: Int) => rdd.take(n).foreach(println)\nval bars = \"---------\"\nval printMsg = (msg:String) => println(bars+msg+bars)\n","user":"anonymous","dateUpdated":"2020-04-06T19:54:39+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"bars: String = ---------\nprintMsg: String => Unit = <function1>\n"}]},"apps":[],"jobName":"paragraph_1584459570162_1575431011","id":"20190519-192640_1954412488","dateCreated":"2020-03-17T15:39:30+0000","dateStarted":"2020-04-06T19:54:39+0000","dateFinished":"2020-04-06T19:54:40+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:368"},{"text":"%md\n### Spark RDD Transormations and Actions\n\n- <a href=http://spark.apache.org/docs/2.3.0/rdd-programming-guide.html#transformations target=\"_blank\">Transformations</a>\n- <a href=http://spark.apache.org/docs/2.3.0/rdd-programming-guide.html#actions target=\"_blank\">Actions</a>\n- Databrick RDD operations http://bit.ly/30ez9IG\n- Spark: The Definitive Guide Chapter 12 (required) & Chapter 13 (Optional)\n\n#### RDD Actions\n1. Get the first element from `retailRDD`\n2. Get the first 5 elements from `retailRDD` as an array.\n3. Get all elements from `retailRDD` as an array\n4. Get random 5 elements from `retailRDD` as an array\n5. Save all elements from `retailRDD` to local file `hdfs:///tmp/text.txt`\n\nSample outputs:\n```bash\n#1\n536365,85123A,WHITE HANGING HEART T-LIGHT HOLDER,6,12/1/2010 8:26,2.55,17850,United Kingdom\n#2\n536365,85123A,WHITE HANGING HEART T-LIGHT HOLDER,6,12/1/2010 8:26,2.55,17850,United Kingdom\n536365,71053,WHITE METAL LANTERN,6,12/1/2010 8:26,3.39,17850,United Kingdom\n536365,84406B,CREAM CUPID HEARTS COAT HANGER,8,12/1/2010 8:26,2.75,17850,United Kingdom\n536365,84029G,KNITTED UNION FLAG HOT WATER BOTTLE,6,12/1/2010 8:26,3.39,17850,United Kingdom\n536365,84029E,RED WOOLLY HOTTIE WHITE HEART.,6,12/1/2010 8:26,3.39,17850,United Kingdom\n#3\n541909\n#4\n575477,85099F,JUMBO BAG STRAWBERRY,2,11/9/2011 16:14,4.13,,United Kingdom\n574536,22988,SOLDIERS EGG CUP ,5,11/4/2011 14:35,1.25,15707,United Kingdom\n538104,22748,POPPYS PLAYHOUSE KITCHEN,2,12/9/2010 15:16,2.1,17950,United Kingdom\n555150,84991,60 TEATIME FAIRY CAKE CASES,24,5/31/2011 15:53,0.55,,United Kingdom\n550496,22497,SET OF 2 TINS VINTAGE BATHROOM ,1,4/18/2011 15:05,8.29,,United Kingdom\n#5\nedward@jarvis-hadoop-m:~/BigData$ hdfs dfs -ls /tmp\ndrwxr-xr-x   - zeppelin hadoop          0 2019-09-17 18:20 /tmp/test.txt\n\n```\n\n","user":"anonymous","dateUpdated":"2020-03-19T22:44:41+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Spark RDD Transormations and Actions</h3>\n<ul>\n  <li>\n  <a href=http://spark.apache.org/docs/2.3.0/rdd-programming-guide.html#transformations target=\"_blank\">Transformations</a></li>\n  <li>\n  <a href=http://spark.apache.org/docs/2.3.0/rdd-programming-guide.html#actions target=\"_blank\">Actions</a></li>\n  <li>Databrick RDD operations <a href=\"http://bit.ly/30ez9IG\">http://bit.ly/30ez9IG</a></li>\n  <li>Spark: The Definitive Guide Chapter 12 (required) &amp; Chapter 13 (Optional)</li>\n</ul>\n<h4>RDD Actions</h4>\n<ol>\n  <li>Get the first element from <code>retailRDD</code></li>\n  <li>Get the first 5 elements from <code>retailRDD</code> as an array.</li>\n  <li>Get all elements from <code>retailRDD</code> as an array</li>\n  <li>Get random 5 elements from <code>retailRDD</code> as an array</li>\n  <li>Save all elements from <code>retailRDD</code> to local file <code>hdfs:///tmp/text.txt</code></li>\n</ol>\n<p>Sample outputs:</p>\n<pre><code class=\"bash\">#1\n536365,85123A,WHITE HANGING HEART T-LIGHT HOLDER,6,12/1/2010 8:26,2.55,17850,United Kingdom\n#2\n536365,85123A,WHITE HANGING HEART T-LIGHT HOLDER,6,12/1/2010 8:26,2.55,17850,United Kingdom\n536365,71053,WHITE METAL LANTERN,6,12/1/2010 8:26,3.39,17850,United Kingdom\n536365,84406B,CREAM CUPID HEARTS COAT HANGER,8,12/1/2010 8:26,2.75,17850,United Kingdom\n536365,84029G,KNITTED UNION FLAG HOT WATER BOTTLE,6,12/1/2010 8:26,3.39,17850,United Kingdom\n536365,84029E,RED WOOLLY HOTTIE WHITE HEART.,6,12/1/2010 8:26,3.39,17850,United Kingdom\n#3\n541909\n#4\n575477,85099F,JUMBO BAG STRAWBERRY,2,11/9/2011 16:14,4.13,,United Kingdom\n574536,22988,SOLDIERS EGG CUP ,5,11/4/2011 14:35,1.25,15707,United Kingdom\n538104,22748,POPPYS PLAYHOUSE KITCHEN,2,12/9/2010 15:16,2.1,17950,United Kingdom\n555150,84991,60 TEATIME FAIRY CAKE CASES,24,5/31/2011 15:53,0.55,,United Kingdom\n550496,22497,SET OF 2 TINS VINTAGE BATHROOM ,1,4/18/2011 15:05,8.29,,United Kingdom\n#5\nedward@jarvis-hadoop-m:~/BigData$ hdfs dfs -ls /tmp\ndrwxr-xr-x   - zeppelin hadoop          0 2019-09-17 18:20 /tmp/test.txt\n\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1584459570162_-46818678","id":"20190519-115905_2023471169","dateCreated":"2020-03-17T15:39:30+0000","dateStarted":"2020-03-19T22:44:41+0000","dateFinished":"2020-03-19T22:44:41+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:369"},{"text":"//first element from retailRDD\nprintln(\"#1\")\nval header = retailRDD.first()\nval rows = retailRDD.filter(line => line != header)\nprintln(rows.take(1).foreach(println))\n\n//Get the first 5 elements from retailRDD as an array\nprintln(\"#2\")\nprintln(rows.take(5).foreach(println))\n\n//Get all elements from retailRDD as an array(including header)\nprintln(\"#3\")\nprintln(retailRDD.collect.length)\n\n//Get random 5 elements from retailRDD as an array\nprintln(\"#4\")\nretailRDD.takeSample(false, 5, 5).foreach(println)\n\n//Save all elements from retailRDD to local file hdfs:///tmp/text.txt\nprintln(\"#5\")\n//retailRDD.saveAsTextFile(\"hdfs:///tmp/text.txt\")","user":"anonymous","dateUpdated":"2020-04-06T19:54:44+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{"0":{"graph":{"mode":"table","height":402.246,"optionOpen":false}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"#1\nheader: String = InvoiceNo,StockCode,Description,Quantity,InvoiceDate,UnitPrice,CustomerID,Country\nrows: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[32] at filter at <console>:31\n536365,85123A,WHITE HANGING HEART T-LIGHT HOLDER,6,12/1/2010 8:26,2.55,17850,United Kingdom\n()\n#2\n536365,85123A,WHITE HANGING HEART T-LIGHT HOLDER,6,12/1/2010 8:26,2.55,17850,United Kingdom\n536365,71053,WHITE METAL LANTERN,6,12/1/2010 8:26,3.39,17850,United Kingdom\n536365,84406B,CREAM CUPID HEARTS COAT HANGER,8,12/1/2010 8:26,2.75,17850,United Kingdom\n536365,84029G,KNITTED UNION FLAG HOT WATER BOTTLE,6,12/1/2010 8:26,3.39,17850,United Kingdom\n536365,84029E,RED WOOLLY HOTTIE WHITE HEART.,6,12/1/2010 8:26,3.39,17850,United Kingdom\n()\n#3\n541910\n#4\n569218,35970,ZINC FOLKART SLEIGH BELLS,6,10/2/2011 12:47,1.69,15952,United Kingdom\n562213,22489,PACK OF 12 TRADITIONAL CRAYONS,6,8/3/2011 13:39,0.42,13975,United Kingdom\n567901,21211,SET OF 72 SKULL PAPER  DOILIES,1,9/22/2011 16:28,2.92,,United Kingdom\nC580971,22313,OFFICE MUG WARMER PINK,-250,12/6/2011 15:04,2.55,14298,United Kingdom\n578827,22993,SET OF 4 PANTRY JELLY MOULDS,1,11/25/2011 14:25,2.46,,United Kingdom\n#5\n"}]},"apps":[],"jobName":"paragraph_1584459570165_535517919","id":"20190519-122034_629713430","dateCreated":"2020-03-17T15:39:30+0000","dateStarted":"2020-04-06T19:54:44+0000","dateFinished":"2020-04-06T19:54:50+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:370"},{"text":"%md","user":"anonymous","dateUpdated":"2020-03-17T15:39:30+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1584459570166_-273217446","id":"20190922-215221_1578966852","dateCreated":"2020-03-17T15:39:30+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:371"},{"text":"%md\n#### RDD Transformations\nRDD Transformations are lazy eval. You can trigger and verify you RDD with a action (e.g. `printRddNSamples` uses `takeSample` action) \n\n1. Get all sales from \"United Kingdom\" (hint: use fileter)\n2. Compare `sample` and `takeSample`\n\nSampel outputs:\n```bash\n#1\n495478\n495478\n495478\n#2\nrdd.sample vs rdd.takeSample\n```","user":"anonymous","dateUpdated":"2020-03-20T15:04:01+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>RDD Transformations</h4>\n<p>RDD Transformations are lazy eval. You can trigger and verify you RDD with a action (e.g. <code>printRddNSamples</code> uses <code>takeSample</code> action) </p>\n<ol>\n  <li>Get all sales from &ldquo;United Kingdom&rdquo; (hint: use fileter)</li>\n  <li>Compare <code>sample</code> and <code>takeSample</code></li>\n</ol>\n<p>Sampel outputs:</p>\n<pre><code class=\"bash\">#1\n495478\n495478\n495478\n#2\nrdd.sample vs rdd.takeSample\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1584459570166_1026717530","id":"20190917-181850_863623231","dateCreated":"2020-03-17T15:39:30+0000","dateStarted":"2020-03-20T15:04:01+0000","dateFinished":"2020-03-20T15:04:04+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:372"},{"text":"//Get all sales from “United Kingdom”\nprintln(\"#1\")\nval ukSales =  retailRDD.map(row => row.split(\",\")).filter(rec => rec(7) equals \"United Kingdom\")\nprintln(ukSales.count)\n","user":"anonymous","dateUpdated":"2020-04-06T19:54:59+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"#1\nukSales: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[35] at filter at <console>:29\n495478\n"}]},"apps":[],"jobName":"paragraph_1584459570167_-893785517","id":"20190519-124053_1683164197","dateCreated":"2020-03-17T15:39:30+0000","dateStarted":"2020-04-06T19:54:59+0000","dateFinished":"2020-04-06T19:55:00+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:373"},{"text":"%md\n### Pair RDD (KeyValue)\nSo far, each element in `retailRDD` is a String. However, many distributed computation paradigms require KV pairs (e.g. MapReduce). Let's construct Pair RDDs from `retailRDD` in order to perform more advanced computations.\n\n**RDD vs PairRDD**:\n\n- https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.RDD\n- https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions","user":"anonymous","dateUpdated":"2020-03-20T20:26:37+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Pair RDD (KeyValue)</h3>\n<p>So far, each element in <code>retailRDD</code> is a String. However, many distributed computation paradigms require KV pairs (e.g. MapReduce). Let&rsquo;s construct Pair RDDs from <code>retailRDD</code> in order to perform more advanced computations.</p>\n<p><strong>RDD vs PairRDD</strong>:</p>\n<ul>\n  <li><a href=\"https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.RDD\">https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.RDD</a></li>\n  <li><a href=\"https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions\">https://spark.apache.org/docs/2.3.3/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions</a></li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1584459570168_-783634896","id":"20190519-125017_38292448","dateCreated":"2020-03-17T15:39:30+0000","dateStarted":"2020-03-20T20:26:37+0000","dateFinished":"2020-03-20T20:26:37+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:374"},{"text":"%md\n\n#### Questions 1.0\n\nTransform each element in `retailRDD` to a key value pair (as a tuple) as following\n\n```\nkey = country\nvalue = amount (e.g. Quantity * UnitPrice)\n```\n\nhint: \n\n- use `rdd.map`\n- Use `row.split(\",\")` to tokenize the row\n- Cast quanitity to int while parsing the row\n- Cast price to double\n\n**Sample output**:\n\n```\n//resultRdd.takeSample(false, 3,3)\n//wehere (key=country, value=amount) for each tuple\nres405: Array[(String, Double)] = Array((Germany,15.0), (United Kingdom,8.26), (United Kingdom,4.13))\n```\n\n","user":"anonymous","dateUpdated":"2020-03-21T02:37:19+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Questions 1.0</h4>\n<p>Transform each element in <code>retailRDD</code> to a key value pair (as a tuple) as following</p>\n<pre><code>key = country\nvalue = amount (e.g. Quantity * UnitPrice)\n</code></pre>\n<p>hint: </p>\n<ul>\n  <li>use <code>rdd.map</code></li>\n  <li>Use <code>row.split(&quot;,&quot;)</code> to tokenize the row</li>\n  <li>Cast quanitity to int while parsing the row</li>\n  <li>Cast price to double</li>\n</ul>\n<p><strong>Sample output</strong>:</p>\n<pre><code>//resultRdd.takeSample(false, 3,3)\n//wehere (key=country, value=amount) for each tuple\nres405: Array[(String, Double)] = Array((Germany,15.0), (United Kingdom,8.26), (United Kingdom,4.13))\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1584459570170_-104904009","id":"20190519-195132_1947538683","dateCreated":"2020-03-17T15:39:30+0000","dateStarted":"2020-03-21T02:37:19+0000","dateFinished":"2020-03-21T02:37:19+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:375"},{"text":"//Question 1.0 solution\n\nval parseKeyValue = (row: String ) => {\n    val tokens = row.split(\",\")\n    val country= tokens.last\n    val quantity = tokens(3)\n    val unitPrice = tokens(5)\n    val amount = quantity.toInt * unitPrice.toDouble    \n    (country, amount)\n}\n\n// val header = retailRDD.first()\n// val rows = retailRDD.filter(line => line != header)\nval ctryRdd = rows.map(parseKeyValue)\nctryRdd.takeSample(false ,3,3)","user":"anonymous","dateUpdated":"2020-04-06T19:55:01+0000","config":{"lineNumbers":true,"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{"0":{"graph":{"mode":"table","height":822,"optionOpen":false}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"parseKeyValue: String => (String, Double) = <function1>\nctryRdd: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[36] at map at <console>:38\nres77: Array[(String, Double)] = Array((Germany,19.799999999999997), (United Kingdom,8.26), (United Kingdom,4.13))\n"}]},"apps":[],"jobName":"paragraph_1584459570174_46428022","id":"20190519-125921_348001552","dateCreated":"2020-03-17T15:39:30+0000","dateStarted":"2020-04-06T19:55:02+0000","dateFinished":"2020-04-06T19:55:04+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:376"},{"user":"anonymous","dateUpdated":"2020-03-17T15:39:30+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1584459570174_956685909","id":"20190922-215540_2030793994","dateCreated":"2020-03-17T15:39:30+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:377"},{"text":"%md\n\n#### Questions 1.1\n\nCalculate total sales amount for each country and sort in descending order\n\n```sql\nSELECT country, sum(Quantity * UnitPrice) as total_sales\nFROM retail\nGROUP BY country\nORDER BY total_sales\n```\n\n**Sample output**\n\n```bash\n//resultRdd.take(3)\n//key=country, value=total_sales\nres415: Array[(String, Double)] = Array((France,197403.90000000026), (Malta,2505.470000000001), (Greece,4710.5199999999995))\n```\n\nHints:\n\n- implement `group by` with `rdd.reduceByKey` [doc](http://bit.ly/30fJHHs)\n- implement `order by` with `rdd.sortBy`","user":"anonymous","dateUpdated":"2020-03-22T05:17:34+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Questions 1.1</h4>\n<p>Calculate total sales amount for each country and sort in descending order</p>\n<pre><code class=\"sql\">SELECT country, sum(Quantity * UnitPrice) as total_sales\nFROM retail\nGROUP BY country\nORDER BY total_sales\n</code></pre>\n<p><strong>Sample output</strong></p>\n<pre><code class=\"bash\">//resultRdd.take(3)\n//key=country, value=total_sales\nres415: Array[(String, Double)] = Array((France,197403.90000000026), (Malta,2505.470000000001), (Greece,4710.5199999999995))\n</code></pre>\n<p>Hints:</p>\n<ul>\n  <li>implement <code>group by</code> with <code>rdd.reduceByKey</code> <a href=\"http://bit.ly/30fJHHs\">doc</a></li>\n  <li>implement <code>order by</code> with <code>rdd.sortBy</code></li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1584459570175_1700555685","id":"20190519-195238_1235609517","dateCreated":"2020-03-17T15:39:30+0000","dateStarted":"2020-03-22T05:17:34+0000","dateFinished":"2020-03-22T05:17:36+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:378"},{"text":"%spark\n\n//total sales amount for each country and sort in descending order\nval resultRdd = ctryRdd.reduceByKey((a: Double, b:Double) =>a+b).sortBy(_._2, ascending= false)\nresultRdd.take(3).foreach(println)","user":"anonymous","dateUpdated":"2020-04-06T19:55:08+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"resultRdd: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[43] at sortBy at <console>:39\n(United Kingdom,8187806.363998696)\n(Netherlands,284661.539999999)\n(EIRE,263276.81999999884)\n"}]},"apps":[],"jobName":"paragraph_1584459570176_-370941471","id":"20190519-195236_1582695577","dateCreated":"2020-03-17T15:39:30+0000","dateStarted":"2020-04-06T19:55:08+0000","dateFinished":"2020-04-06T19:55:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:379"},{"user":"anonymous","dateUpdated":"2020-03-17T15:39:30+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1584459570177_1510637666","id":"20190922-215553_1555963294","dateCreated":"2020-03-17T15:39:30+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:380"},{"text":"%md\n\n#### Questions 2.0\n\nLet's assume CustomerID is auto-increment. For each country, find the earliest registered customer (e.g. smallest CustomerID)\n\n```sql\nSELECT min(CustomerID), Country\nFROM retail\nGROUP BY Country\n```\n\n**Sample output**\n\n```\n//resultRdd.collect\nArray[(String, Int)] = Array((Australia,12386), (Portugal,12356), (United Kingdom,12346), (Brazil,12769), (Canada,15388), (Japan,12753), (Cyprus,12359), (European Community,15108), (Finland,12348), (Iceland,12347), (Netherlands,12759), (Singapore,12744), (Sweden,12483), (RSA,12446), (Norway,12350), (Denmark,12367), (Poland,12576), (Israel,12512), (Saudi Arabia,12565), (Belgium,12361), (Lithuania,15332), (Greece,12478), (Italy,12349), (France,12413), (Switzerland,12357), (Spain,12354), (USA,12558), (Germany,12426), (United Arab Emirates,12739), (EIRE,14016), (Hong Kong,2147483647), (Bahrain,12353), (Malta,15480), (Unspecified,12363), (Channel Islands,14442), (Austria,12358), (Lebanon,12764), (Czech Republic,12781))\n```\n\n**hints:**\n\n- Generate a new KV pair RDD `(country, id)`\n- use `rdd.reduceByKey` find the smallest\n- ID must be a numric number\n","user":"anonymous","dateUpdated":"2020-03-25T19:59:41+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Questions 2.0</h4>\n<p>Let&rsquo;s assume CustomerID is auto-increment. For each country, find the earliest registered customer (e.g. smallest CustomerID)</p>\n<pre><code class=\"sql\">SELECT min(CustomerID), Country\nFROM retail\nGROUP BY Country\n</code></pre>\n<p><strong>Sample output</strong></p>\n<pre><code>//resultRdd.collect\nArray[(String, Int)] = Array((Australia,12386), (Portugal,12356), (United Kingdom,12346), (Brazil,12769), (Canada,15388), (Japan,12753), (Cyprus,12359), (European Community,15108), (Finland,12348), (Iceland,12347), (Netherlands,12759), (Singapore,12744), (Sweden,12483), (RSA,12446), (Norway,12350), (Denmark,12367), (Poland,12576), (Israel,12512), (Saudi Arabia,12565), (Belgium,12361), (Lithuania,15332), (Greece,12478), (Italy,12349), (France,12413), (Switzerland,12357), (Spain,12354), (USA,12558), (Germany,12426), (United Arab Emirates,12739), (EIRE,14016), (Hong Kong,2147483647), (Bahrain,12353), (Malta,15480), (Unspecified,12363), (Channel Islands,14442), (Austria,12358), (Lebanon,12764), (Czech Republic,12781))\n</code></pre>\n<p><strong>hints:</strong></p>\n<ul>\n  <li>Generate a new KV pair RDD <code>(country, id)</code></li>\n  <li>use <code>rdd.reduceByKey</code> find the smallest</li>\n  <li>ID must be a numric number</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1584459570177_1802816871","id":"20190519-195157_1405617071","dateCreated":"2020-03-17T15:39:30+0000","dateStarted":"2020-03-25T19:59:41+0000","dateFinished":"2020-03-25T19:59:44+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:381"},{"text":"%spark\n//earliest registered customer\nval idRDD = rows.filter(x => x.split(\",\")(6)!=\"\").map(x => {\n    val tokens = x.split(\",\")\n    val country= tokens.last\n    val custID = tokens(6).toInt\n    (country, custID)\n})\n\nval earlCust = idRDD.reduceByKey((a: Int, b:Int) => if(a<b)a else b)\nearlCust.collect","user":"anonymous","dateUpdated":"2020-04-06T19:55:15+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"idRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[45] at map at <console>:35\nearlCust: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[46] at reduceByKey at <console>:35\nres83: Array[(String, Int)] = Array((Australia,12386), (Portugal,12356), (United Kingdom,12346), (Brazil,12769), (Canada,15388), (Japan,12753), (Cyprus,12359), (European Community,15108), (Finland,12348), (Iceland,12347), (Netherlands,12759), (Singapore,12744), (Sweden,12483), (RSA,12446), (Norway,12350), (Denmark,12367), (Poland,12576), (Israel,12512), (Saudi Arabia,12565), (Belgium,12361), (Lithuania,15332), (Greece,12478), (Italy,12349), (France,12413), (Switzerland,12357), (Spain,12354), (USA,12558), (Germany,12426), (United Arab Emirates,12739), (EIRE,14016), (Bahrain,12353), (Malta,15480), (Unspecified,12363), (Channel Islands,14442), (Austria,12358), (Lebanon,12764), (Czech Republic,12781))\n"}]},"apps":[],"jobName":"paragraph_1584459570177_-1086055055","id":"20190519-144439_1578143376","dateCreated":"2020-03-17T15:39:30+0000","dateStarted":"2020-04-06T19:55:15+0000","dateFinished":"2020-04-06T19:55:18+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:382"},{"user":"anonymous","dateUpdated":"2020-03-17T15:39:30+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1584459570178_1071610087","id":"20190922-215609_1113478409","dateCreated":"2020-03-17T15:39:30+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:383"},{"text":"%md\n\n### Question 2.1\n\nIt's inconvenient to tokenize each row in every operation. Instead, we count convert `retailRDD[String]` to `itemsRdd:RDD[Item]` where `Item` is a case class as following:\n\n`case class Item(invoiceNo:String, stockCode:String, description:Option[String], quantity:Int, invoiceDate:String, unitPrice:Double, customerID:Option[Int], Country:String)`\n\nIn this way, you only parse each row only once here and you can reuse `itemsRdd` in the rest of the questions.\n\n**Sample outputs**\n```scala\n//itemsRdd.take(3)\nres445: Array[Item] = Array(Item(536365,85123A,Some(WHITE HANGING HEART T-LIGHT HOLDER),6,12/1/2010 8:26,2.55,Some(17850),United Kingdom), Item(536365,71053,Some(WHITE METAL LANTERN),6,12/1/2010 8:26,3.39,Some(17850),United Kingdom), Item(536365,84406B,Some(CREAM CUPID HEARTS COAT HANGER),8,12/1/2010 8:26,2.75,Some(17850),United Kingdom))\n```\n\n**Hints**:\n\n- write a function to convert a row to a Item, e.g. \n```\nval parseRow2Item: (String) => Item = (row: String) => {\n    //complete body\n}\n```\n- Covert all rows to items, e.g. `val itemsRdd =  retailRDD.map(parseRow2Item)`\n","user":"anonymous","dateUpdated":"2020-03-21T19:15:07+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Question 2.1</h3>\n<p>It&rsquo;s inconvenient to tokenize each row in every operation. Instead, we count convert <code>retailRDD[String]</code> to <code>itemsRdd:RDD[Item]</code> where <code>Item</code> is a case class as following:</p>\n<p><code>case class Item(invoiceNo:String, stockCode:String, description:Option[String], quantity:Int, invoiceDate:String, unitPrice:Double, customerID:Option[Int], Country:String)</code></p>\n<p>In this way, you only parse each row only once here and you can reuse <code>itemsRdd</code> in the rest of the questions.</p>\n<p><strong>Sample outputs</strong></p>\n<pre><code class=\"scala\">//itemsRdd.take(3)\nres445: Array[Item] = Array(Item(536365,85123A,Some(WHITE HANGING HEART T-LIGHT HOLDER),6,12/1/2010 8:26,2.55,Some(17850),United Kingdom), Item(536365,71053,Some(WHITE METAL LANTERN),6,12/1/2010 8:26,3.39,Some(17850),United Kingdom), Item(536365,84406B,Some(CREAM CUPID HEARTS COAT HANGER),8,12/1/2010 8:26,2.75,Some(17850),United Kingdom))\n</code></pre>\n<p><strong>Hints</strong>:</p>\n<ul>\n  <li>\n  <p>write a function to convert a row to a Item, e.g. </p>\n  <pre><code>val parseRow2Item: (String) =&gt; Item = (row: String) =&gt; {\n//complete body\n}\n</code></pre></li>\n  <li>Covert all rows to items, e.g. <code>val itemsRdd =  retailRDD.map(parseRow2Item)</code></li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1584459570178_1778353207","id":"20190921-180308_1963749922","dateCreated":"2020-03-17T15:39:30+0000","dateStarted":"2020-03-21T19:15:07+0000","dateFinished":"2020-03-21T19:15:07+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:384"},{"text":"%spark\n// converting a row to a Item\ncase class Item(invoiceNo:String, stockCode:String, description:Option[String], quantity:Int, invoiceDate:String, unitPrice:Double, customerID:Option[Int], Country:String)\n\nval parseRow2Item: (String) => Item = (row: String) => {\n    val tokens = row.split(\",\")\n    val custId = if(tokens(6).equals(\"\")) None else Some(tokens(6).toInt)\n    val description = if(tokens(2).equals(\"\")) None else Some(tokens(2))\n    val country = tokens.last\n\n//Coverting all rows to items    \n    val item = Item(tokens(0),tokens(1), description,tokens(3).toInt, tokens(4),tokens(5).toDouble, custId, country)\n    (item)\n}\n\n//\nval itemsRdd = rows.map(parseRow2Item)\nitemsRdd.take(3)","user":"anonymous","dateUpdated":"2020-04-06T19:55:22+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"defined class Item\nparseRow2Item: String => Item = <function1>\nitemsRdd: org.apache.spark.rdd.RDD[Item] = MapPartitionsRDD[47] at map at <console>:39\nres87: Array[Item] = Array(Item(536365,85123A,Some(WHITE HANGING HEART T-LIGHT HOLDER),6,12/1/2010 8:26,2.55,Some(17850),United Kingdom), Item(536365,71053,Some(WHITE METAL LANTERN),6,12/1/2010 8:26,3.39,Some(17850),United Kingdom), Item(536365,84406B,Some(CREAM CUPID HEARTS COAT HANGER),8,12/1/2010 8:26,2.75,Some(17850),United Kingdom))\n"}]},"apps":[],"jobName":"paragraph_1584459570179_-1113261312","id":"20190921-180433_1776305327","dateCreated":"2020-03-17T15:39:30+0000","dateStarted":"2020-04-06T19:55:22+0000","dateFinished":"2020-04-06T19:55:23+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:385"},{"text":"%spark\n","user":"anonymous","dateUpdated":"2020-03-22T05:15:47+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1584854147184_-2009086982","id":"20200322-051547_214364282","dateCreated":"2020-03-22T05:15:47+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:386"},{"text":"%md\n### Questions 2.2\n\nRe-implement questions 1.1 & 2.1 using itemsRdd (`RDD[Item]`)","user":"anonymous","dateUpdated":"2020-03-17T15:39:30+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Questions 2.2</h3>\n<p>Re-implement questions 1.1 &amp; 2.1 using itemsRdd (<code>RDD[Item]</code>)</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1584459570180_-1853646592","id":"20190922-140917_1244358721","dateCreated":"2020-03-17T15:39:30+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:387"},{"text":"%spark\nprintln(\"#1.1\")\nval countryMap = itemsRdd.map((x: Item) => {\n  (x.Country, x.unitPrice * x.quantity)  \n})\ncountryMap.take(2)\nval resultItemRdd = countryMap.reduceByKey((a: Double, b:Double) =>a+b).sortBy(_._2, false)\nresultRdd.take(3)\n\nprintln(\"#2.1\")\n","user":"anonymous","dateUpdated":"2020-04-06T19:55:29+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{"0":{"graph":{"mode":"table","height":308,"optionOpen":false}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"#1.1\ncountryMap: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[48] at map at <console>:40\nres90: Array[(String, Double)] = Array((United Kingdom,15.299999999999999), (United Kingdom,20.34))\nresultItemRdd: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[54] at sortBy at <console>:41\nres91: Array[(String, Double)] = Array((United Kingdom,8187806.363998696), (Netherlands,284661.539999999), (EIRE,263276.81999999884))\n#2.1\n"}]},"apps":[],"jobName":"paragraph_1584459570180_-874885530","id":"20190922-141025_1178356031","dateCreated":"2020-03-17T15:39:30+0000","dateStarted":"2020-04-06T19:55:29+0000","dateFinished":"2020-04-06T19:55:32+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:388"},{"text":"%spark\n","user":"anonymous","dateUpdated":"2020-03-17T15:39:30+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1584459570181_-8045601","id":"20190922-215642_1393968112","dateCreated":"2020-03-17T15:39:30+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:389"},{"text":"%md\n#### Questions 3\n\nFind number of customers.\n\n```\nSELECT distinct(customerId)\nFROM retail\n```\n\n**Sample output**\n```scala\n//resultRdd.count\nres458: Long = 4373\n```","user":"anonymous","dateUpdated":"2020-03-21T20:16:59+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Questions 3</h4>\n<p>Find number of customers.</p>\n<pre><code>SELECT distinct(customerId)\nFROM retail\n</code></pre>\n<p><strong>Sample output</strong></p>\n<pre><code class=\"scala\">//resultRdd.count\nres458: Long = 4373\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1584459570181_-475603198","id":"20190519-195407_556558321","dateCreated":"2020-03-17T15:39:30+0000","dateStarted":"2020-03-21T20:16:59+0000","dateFinished":"2020-03-21T20:16:59+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:390"},{"text":"%spark\n\n// finfing number of customers\nval custRDD = rows.map(x => {\n    val tokens = x.split(\",\")\n    val custID= tokens(6)\n    (custID)\n})\n\ncustRDD.distinct().count()\n","user":"anonymous","dateUpdated":"2020-04-06T19:55:36+0000","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"custRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[55] at map at <console>:35\nres95: Long = 4373\n"}]},"apps":[],"jobName":"paragraph_1584459570182_1968140848","id":"20190519-194831_1486531342","dateCreated":"2020-03-17T15:39:30+0000","dateStarted":"2020-04-06T19:55:36+0000","dateFinished":"2020-04-06T19:55:37+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:391"},{"user":"anonymous","dateUpdated":"2020-03-17T15:39:30+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1584459570182_-362807809","id":"20190922-215651_1847295545","dateCreated":"2020-03-17T15:39:30+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:392"},{"text":"%md\n#### Question 4\n\nFind out the number of invoices/purchases for each customer.\n> Note: a invoiceNo can appear in multiple rows (e.g. a invoice contains more than one item)\n\n```sql\nSELECT CustomerId, count(distinct(invoiceNo)) as purchases\nFROM retail\nGROUP by CustomerId\n```\n\n**Sample output**\n```scala\n//rdd.takeSample(false,3,3)\n//where (key=customerId, value=purchases) for each tuple\nres509: Array[(Any, Int)] = Array((16305,1), (16804,3), (16226,3))\n```\n","user":"anonymous","dateUpdated":"2020-03-22T06:21:52+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Question 4</h4>\n<p>Find out the number of invoices/purchases for each customer.</p>\n<blockquote>\n  <p>Note: a invoiceNo can appear in multiple rows (e.g. a invoice contains more than one item)</p>\n</blockquote>\n<pre><code class=\"sql\">SELECT CustomerId, count(distinct(invoiceNo)) as purchases\nFROM retail\nGROUP by CustomerId\n</code></pre>\n<p><strong>Sample output</strong></p>\n<pre><code class=\"scala\">//rdd.takeSample(false,3,3)\n//where (key=customerId, value=purchases) for each tuple\nres509: Array[(Any, Int)] = Array((16305,1), (16804,3), (16226,3))\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1584459570182_1497200655","id":"20190519-195310_661372203","dateCreated":"2020-03-17T15:39:30+0000","dateStarted":"2020-03-22T06:21:52+0000","dateFinished":"2020-03-22T06:21:52+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:393"},{"text":"%spark\n//number of invoices/purchases for each customer.\nval rdd = itemsRdd.map(x => (x.customerID, x.invoiceNo))\n\nval distinctRdd = rdd.distinct().mapValues(_ => 1).reduceByKey(_ + _);\ndistinctRdd.takeSample(false,3,3)\n","user":"anonymous","dateUpdated":"2020-04-06T19:55:42+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"rdd: org.apache.spark.rdd.RDD[(Option[Int], String)] = MapPartitionsRDD[59] at map at <console>:41\ndistinctRdd: org.apache.spark.rdd.RDD[(Option[Int], Int)] = ShuffledRDD[64] at reduceByKey at <console>:41\nres98: Array[(Option[Int], Int)] = Array((Some(14451),2), (Some(16581),2), (Some(13157),5))\n"}]},"apps":[],"jobName":"paragraph_1584459570183_294700506","id":"20190922-190215_1478690977","dateCreated":"2020-03-17T15:39:30+0000","dateStarted":"2020-04-06T19:55:42+0000","dateFinished":"2020-04-06T19:55:45+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:394"},{"text":"%md\n#### SPARK UI\nEvery Spark Job has a web UI for monitroing and debuging purposes.\nGo to GCP > your hadoop cluster > web interfaces > Spark History Server > Spark UI. ","user":"anonymous","dateUpdated":"2020-03-17T15:39:30+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>SPARK UI</h4>\n<p>Every Spark Job has a web UI for monitroing and debuging purposes.<br/>Go to GCP &gt; your hadoop cluster &gt; web interfaces &gt; Spark History Server &gt; Spark UI.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1584459570183_-350746343","id":"20190521-114127_1095254606","dateCreated":"2020-03-17T15:39:30+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:395"},{"text":"%md\n#### RDD join\n\nPrint `customerId, name, country` using `customers.txt` and  `online-retail-dataset_clean.txt` files\n\n1. Load `datasets/online_retail/customers.txt` to `RDD[Customer]` where `Customer` is a case class as following\n`case class Customer(customerId:Int, name: String)`\n2. Join `RDD[Customer]` with `RDD[item]` on `customerId`\n3. Select uniq `customerId, name, country`\n\n```sql\nSELECT retail.customerID, customer.name, country\nFROM customer LEFT JOIN retail ON customer.id = retail.customerId\n```\n\n**Sample Output**\n```bash\n//resultRdd.take(3)\nres553: Array[(Int, String, String)] = Array((13311,Petra M. Dalton,United Kingdom), (12367,Wade S. Blair,Denmark), (18144,Gage A. Sharpe,United Kingdom))\n```","user":"anonymous","dateUpdated":"2020-03-22T06:31:23+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>RDD join</h4>\n<p>Print <code>customerId, name, country</code> using <code>customers.txt</code> and <code>online-retail-dataset_clean.txt</code> files</p>\n<ol>\n  <li>Load <code>datasets/online_retail/customers.txt</code> to <code>RDD[Customer]</code> where <code>Customer</code> is a case class as following<br/><code>case class Customer(customerId:Int, name: String)</code></li>\n  <li>Join <code>RDD[Customer]</code> with <code>RDD[item]</code> on <code>customerId</code></li>\n  <li>Select uniq <code>customerId, name, country</code></li>\n</ol>\n<pre><code class=\"sql\">SELECT retail.customerID, customer.name, country\nFROM customer LEFT JOIN retail ON customer.id = retail.customerId\n</code></pre>\n<p><strong>Sample Output</strong></p>\n<pre><code class=\"bash\">//resultRdd.take(3)\nres553: Array[(Int, String, String)] = Array((13311,Petra M. Dalton,United Kingdom), (12367,Wade S. Blair,Denmark), (18144,Gage A. Sharpe,United Kingdom))\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1584459570184_1453441880","id":"20190519-183851_617743118","dateCreated":"2020-03-17T15:39:30+0000","dateStarted":"2020-03-22T06:31:23+0000","dateFinished":"2020-03-22T06:31:23+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:396"},{"text":"%spark\n\nprintln(\"#5.1\")\ncase class Customer(customerId:Int, name: String)\nval custData = sc.textFile(\"hdfs:///user/manpreetk0294/datasets/online_retail/customers.txt\")\nval custRDD = custData.map(row => {\n    val values = row.split(\",\")\n    Customer(values(0).toInt, values(1))\n})\n\nprintln(\"#5.2\")\nval custInvoiceRDD = custRDD.keyBy(_.customerId).leftOuterJoin(itemsRdd.filter(item => item.customerID.nonEmpty).keyBy(_.customerID.get))\n\nprintln(\"#5.3\")\nval custInfoRDD = custInvoiceRDD.map(x=>{\n    val id = x._1\n    val customer = x._2._1\n    val item = x._2._2.getOrElse(Item(\"\",\"\",None,0,\"\",0.0,Some(id),\"NA\"))\n    (id,customer.name, item.Country)\n}).distinct\ncustInfoRDD.takeSample(false, 5).foreach(println)","user":"anonymous","dateUpdated":"2020-04-06T20:05:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"#5.1\ndefined class Customer\ncustData: org.apache.spark.rdd.RDD[String] = hdfs:///user/manpreetk0294/datasets/online_retail/customers.txt MapPartitionsRDD[127] at textFile at <console>:25\ncustRDD: org.apache.spark.rdd.RDD[Customer] = MapPartitionsRDD[128] at map at <console>:30\n#5.2\ncustInvoiceRDD: org.apache.spark.rdd.RDD[(Int, (Customer, Option[Item]))] = MapPartitionsRDD[134] at leftOuterJoin at <console>:45\n#5.3\ncustInfoRDD: org.apache.spark.rdd.RDD[(Int, String, String)] = MapPartitionsRDD[138] at distinct at <console>:53\n(12967, Wade S. Blair,United Kingdom)\n(14395, Brianna A. Nolan,United Kingdom)\n(14027, Irene M. Abbott,United Kingdom)\n(14873, Octavius G. Welch,United Kingdom)\n(13816, Hunter L. Kinney,Germany)\n"}]},"apps":[],"jobName":"paragraph_1584459570184_462201605","id":"20190922-193513_1093352183","dateCreated":"2020-03-17T15:39:30+0000","dateStarted":"2020-04-06T20:05:16+0000","dateFinished":"2020-04-06T20:05:23+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:397"}],"name":"Jarvis/1-SparkRDD_pub","id":"2F6NC1E1J","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"sh:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}